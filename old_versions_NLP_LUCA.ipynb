{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating deep copy of the plots_summaries dataframes creatde above \n",
    "df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "# Verify if laptop running the script has a GPU and CUDA enabled\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu() # enable GPU use for Spacy, taken from: https://stackoverflow.com/questions/75355264/how-to-enable-cuda-gpu-acceleration-for-spacy-on-windows \n",
    "    nlp = spacy.load(\"en_core_web_trf\") # uses a more complex model leveraging transformers\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# \n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries, us_locations):\n",
    "    \"\"\"\n",
    "    Function that implement batch processing of plot summaries and extract GPE entities and frequencies. If available on the machine running the notebook, CUDA is enabled for faster processing.\n",
    "    params: texts called 'summaries' and a set of US locations containing the cities, counties and the states\n",
    "    returns: all the GPE entities and all the US GPE frequencies detected in the summary as a list\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for doc in nlp.pipe(summaries, batch_size=500):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(ent for ent in GPE_entities if ent in us_locations)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "df_plots_us_partially_movies_GPE[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = \\\n",
    "    pd.DataFrame(extract_GPE_and_compute_US_frequencies_batch(\n",
    "        df_plots_us_partially_movies_GPE['Summary'].tolist(), us_locations_set # converts a Pandas Series into a Python list, as nlp.pipe expects a seqeunce of string according to the documentation https://spacy.io/api/language#pipe \n",
    "    ))\n",
    "\n",
    "df_us_partially_movies_NLP_GPE = df_plots_us_partially_movies_GPE.drop(columns=['Summary']) # keeping all the info of the original df, the new GPE columns and dropping the summaries for more clarity and later use\n",
    "\n",
    "df_us_partially_movies_NLP_GPE.to_csv(\"data/us_partially_movies_NLP_GPE.csv\", sep=',', encoding='utf-8', index=False, header = True)# hard code the encoding to avoid anay problems as seen in the lecture\n",
    "\n",
    "print(\"NLP processing done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old GPE tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of US states Github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# List of all us states, counties and cities found on: https://github.com/grammakov/USA-cities-and-states/tree/master\n",
    "df_us_states_counties_cities = pd.read_csv(\"data/us_cities_states_counties.csv\", sep = \"|\")\n",
    "print(f\"The shape of the US states, counties and cities dataset is {df_us_states_counties_cities.shape}.\")\n",
    "\n",
    "# Exclude non-offcial US states that are comprised in the .csv from the Github page\n",
    "exclude_states = [\n",
    "    \"US Armed Forces Pacific\", \"American Samoa\", \"Guam\", \"Palau\",\n",
    "    \"Federated States of Micronesia\", \"Northern Mariana Islands\",\n",
    "    \"Marshall Islands\", \"US Armed Forces Europe\", \"Puerto Rico\", \"Virgin Islands\"\n",
    "]\n",
    "\n",
    "indices_to_drop = df_us_states_counties_cities[\n",
    "    df_us_states_counties_cities['State full'].isin(exclude_states)\n",
    "].index\n",
    "\n",
    "\n",
    "df_us_states_counties_cities.drop(index=indices_to_drop, inplace=True)\n",
    "print(f\"The shape after dropping of the US states, counties and cities dataset is {df_us_states_counties_cities.shape}.\")\n",
    "\n",
    "# Create lists of unique city, county ans states names\n",
    "list_US_states = list(df_us_states_counties_cities['State full'].unique())\n",
    "list_US_counties = list(df_us_states_counties_cities['County'].str.capitalize().unique())\n",
    "list_US_cities = list(df_us_states_counties_cities['City'].unique())\n",
    "\n",
    "# Print the shapes and display only the first 10 locations of each list to avoid too long print statements\n",
    "print(f\"The list of the first ten US states is \\n {list_US_states[:10]} \\n and contains {len(list_US_states)} states.\")\n",
    "print(f\"The list of the first ten US counties is \\n {list_US_counties[:10]} \\n and contains {len(list_US_counties)} counties.\")\n",
    "print(f\"The list of the first ten US cities is \\n {list_US_cities[:10]} \\n and contains {len(list_US_cities)} cities.\")\n",
    "\n",
    "# Combine all US states, counties, and cities into a single set for faster lookup\n",
    "us_locations_set = set(list_US_states + list_US_counties + list_US_cities)\n",
    "\n",
    "df_us_states_counties_cities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_plots_us_only_movies_GPE = plot_summaries_us_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "# Verify if laptop running the script has a GPU and CUDA enabled\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu() # enable GPU use for Spacy, taken from: https://stackoverflow.com/questions/75355264/how-to-enable-cuda-gpu-acceleration-for-spacy-on-windows \n",
    "    nlp = spacy.load(\"en_core_web_trf\") # uses a more complex model leveraging transformers\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# \n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries, us_locations):\n",
    "    \"\"\"\n",
    "    Function that implement batch processing of plot summaries and extract GPE entities and frequencies. If available on the machine running the notebook, CUDA is enabled for faster processing.\n",
    "    params: texts called 'summaries' and a set of US locations containing the cities, counties and the states\n",
    "    returns: all the GPE entities and all the US GPE frequencies detected in the summary as a list\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for doc in nlp.pipe(summaries, batch_size=200):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(ent for ent in GPE_entities if ent in us_locations)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "df_plots_us_only_movies_GPE[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = \\\n",
    "    pd.DataFrame(extract_GPE_and_compute_US_frequencies_batch(\n",
    "        df_plots_us_only_movies_GPE['Summary'].tolist(), us_locations_set # converts a Pandas Series into a Python list, as nlp.pipe expects a seqeunce of string according to the documentation https://spacy.io/api/language#pipe \n",
    "    ))\n",
    "\n",
    "df_us_only_movies_NLP_GPE = df_plots_us_only_movies_GPE.drop(columns=['Summary']) # keeping all the info of the original df, the new GPE columns and dropping the summaries for more clarity and later use\n",
    "\n",
    "df_us_only_movies_NLP_GPE.to_csv(\"data/us_only_movies_NLP_GPE.csv\", sep=',', encoding='utf-8', index=False, header = True)# hard code the encoding to avoid anay problems as seen in the lecture\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu()  # Enable GPU use for SpaCy\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # Use a more complex model leveraging transformers\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Geocode a single location and check if it's in the US\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    locate = geolocator.geocode(location)\n",
    "    check = True if locate and 'United States' in str(locate) else False\n",
    "    return location, check\n",
    "\n",
    "# Geocode multiple locations in parallel\n",
    "def geocode_locations_parallel(locations):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(geocode_location, locations), total=len(locations), desc=\"Geocoding Locations\"))\n",
    "    return results\n",
    "\n",
    "# Extract GPE entities and compute US frequencies with geocoding\n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(summaries, batch_size=500), total=len(summaries), desc=\"Processing Summaries\"):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        geocoded_results = geocode_locations_parallel(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(location for location, is_in_us in geocoded_results if is_in_us)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Example DataFrame setup and processing\n",
    "# Assuming df_plots_us_partially_movies_GPE is already defined\n",
    "df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "df_plots_us_partially_movies_GPE[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = pd.DataFrame(\n",
    "    extract_GPE_and_compute_US_frequencies_batch(\n",
    "        df_plots_us_partially_movies_GPE['Summary'].tolist()\n",
    "    ))\n",
    "\n",
    "# Drop summaries and save results\n",
    "df_us_partially_movies_NLP_GPE = df_plots_us_partially_movies_GPE.drop(columns=['Summary'])\n",
    "df_us_partially_movies_NLP_GPE.to_csv(\"data/NLP_datasets/us_partially_movies_NLP_GPE_test_problems.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST AVEC NOMINATIM Sur les plots problématiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu()  # Enable GPU use for SpaCy\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # Use a more complex model leveraging transformers (cf. https://spacy.io/models/en/)\n",
    "    print(\"Using Spacy English transformer pipeline.\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Using Spacy English pipeline optimized for CPU.\")\n",
    "\n",
    "\n",
    "# Initialize caching dictionary\n",
    "geocode_cache = {}\n",
    "\n",
    "# Geocode a single location and check if it's in the US\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    locate = geolocator.geocode(location)\n",
    "    check = True if locate and 'United States' in str(locate) else False\n",
    "    return location, check\n",
    "\n",
    "# Cached geocoding function tio avoid looking sevarl time for the same location of the geopy API\n",
    "def geocode_location_cached(location):\n",
    "    if location not in geocode_cache:\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                time.sleep(1)  # Respect API rate limits\n",
    "                geocode_cache[location] = geocode_location(location)\n",
    "                break\n",
    "            except GeocoderRateLimited:\n",
    "                print(f\"Rate-limited for location: {location}. Retrying...\")\n",
    "                time.sleep(10)  # Wait before retrying\n",
    "    return geocode_cache[location]\n",
    "\n",
    "# Geocode multiple locations in parallel\n",
    "def geocode_locations_parallel(locations):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(geocode_location_cached, locations))\n",
    "    return results\n",
    "\n",
    "# Extract GPE entities and compute US frequencies with geocoding\n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(summaries, batch_size=500), total=len(summaries), desc=\"Processing Summaries\"):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        geocoded_results = geocode_locations_parallel(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(location for location, is_in_us in geocoded_results if is_in_us)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Example DataFrame setup and processing\n",
    "# Assuming df_plots_us_partially_movies_GPE is already defined\n",
    "# df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Filter DataFrame for selected movie titles\n",
    "selected_titles_2 = [\"The Hunger Games\", \"Dark Water\", \"Meet John Doe\",\"Exodus\"] # \n",
    "filtered_df_2 = plot_summaries_us_movies[plot_summaries_us_movies['title'].isin(selected_titles_2)].copy()\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "# filtered_df[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = pd.DataFrame(\n",
    "#     extract_GPE_and_compute_US_frequencies_batch(\n",
    "#         filtered_df['Summary'].tolist()\n",
    "#     ))\n",
    "\n",
    "extracted_results = extract_GPE_and_compute_US_frequencies_batch(filtered_df_2['Summary'].tolist())\n",
    "filtered_df_2['GPE_entities'] = [res[0] for res in extracted_results]\n",
    "filtered_df_2['GPE_US_frequencies'] = [res[1] for res in extracted_results]\n",
    "filtered_df_2['Percentage_american_culture'] = [res[2] for res in extracted_results]\n",
    "\n",
    "\n",
    "# Drop summaries and save results\n",
    "filtered_df_2 = filtered_df_2.drop(columns=['Summary'])\n",
    "# filtered_df.drop(columns=['Summary'], inplace=True)\n",
    "filtered_df_2.to_csv(\"data/NLP_datasets/us_only_movies_NLP_GPE_test_problems.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n",
    "\n",
    "filtered_df_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu()  # Enable GPU use for SpaCy\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # Use a more complex model leveraging transformers (cf. https://spacy.io/models/en/)\n",
    "    print(\"Using Spacy English transformer pipeline.\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Using Spacy English pipeline optimized for CPU.\")\n",
    "\n",
    "\n",
    "# Initialize caching dictionary\n",
    "geocode_cache = {}\n",
    "\n",
    "# Geocode a single location and check if it's in the US\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    locate = geolocator.geocode(location)\n",
    "    check = True if locate and 'United States' in str(locate) else False\n",
    "    return location, check\n",
    "\n",
    "# Cached geocoding function tio avoid looking sevarl time for the same location of the geopy API\n",
    "def geocode_location_cached(location):\n",
    "    if location not in geocode_cache:\n",
    "        geocode_cache[location] = geocode_location(location)\n",
    "    return geocode_cache[location]\n",
    "\n",
    "# Geocode multiple locations in parallel\n",
    "def geocode_locations_parallel(locations):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(geocode_location_cached, locations))\n",
    "    return results\n",
    "\n",
    "# Extract GPE entities and compute US frequencies with geocoding\n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(summaries, batch_size=500), total=len(summaries), desc=\"Processing Summaries\"):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        geocoded_results = geocode_locations_parallel(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(location for location, is_in_us in geocoded_results if is_in_us)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Example DataFrame setup and processing\n",
    "# Assuming df_plots_us_partially_movies_GPE is already defined\n",
    "# df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Filter DataFrame for selected movie titles\n",
    "selected_titles = [\"Come Back, Africa\", \"A Cry in the Dark\", \"End Game\",\"Eastern Promises\",  \"Sophie's Choice\"] # \n",
    "filtered_df = plot_summaries_us_partially_movies[plot_summaries_us_partially_movies['title'].isin(selected_titles)].copy()\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "# filtered_df[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = pd.DataFrame(\n",
    "#     extract_GPE_and_compute_US_frequencies_batch(\n",
    "#         filtered_df['Summary'].tolist()\n",
    "#     ))\n",
    "\n",
    "extracted_results = extract_GPE_and_compute_US_frequencies_batch(filtered_df['Summary'].tolist())\n",
    "filtered_df['GPE_entities'] = [res[0] for res in extracted_results]\n",
    "filtered_df['GPE_US_frequencies'] = [res[1] for res in extracted_results]\n",
    "filtered_df['Percentage_american_culture'] = [res[2] for res in extracted_results]\n",
    "\n",
    "\n",
    "# Drop summaries and save results\n",
    "filtered_df = filtered_df.drop(columns=['Summary'])\n",
    "# filtered_df.drop(columns=['Summary'], inplace=True)\n",
    "filtered_df.to_csv(\"data/NLP_datasets/us_partially_movies_NLP_GPE_test_problems.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n",
    "\n",
    "print(filtered_df.columns)  \n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test avec small spacy model 'en_core_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Creating deep copy of the plots_summaries dataframes creatde above \n",
    "# df_plots_all_movies_GPE = plot_summaries_all_movies.copy()\n",
    "# df_plots_us_movies_GPE = plot_summaries_us_movies.copy()\n",
    "# df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy() # run en 5'30\"\n",
    "# df_plots_RoW_GPE = plot_summaries_RoW_movies.copy()\n",
    "\n",
    "# # since GPE is part of the NER spacy pipeline, no need to tokenize manually the text before implementing the search for GPE quantities\n",
    "# # def extract_gpe(summary):\n",
    "# #     \"\"\" Function that extract the GPE entities of a plot summary\n",
    "# #     params: a text called 'summary'\n",
    "# #     returns: all the GPE entities detected in the summary\n",
    "# #     \"\"\"\n",
    "# #     doc = nlp(summary)\n",
    "# #     return [ent.text for ent in doc.ents if ent.label_ == 'GPE'] # outputs all detected geopolitical entities, including repeated mentions of the same geographical entitities\n",
    "\n",
    "# # def compute_US_GPE_frequencies(summary, set_us_location):\n",
    "# #     \"\"\" Function that extract the US GPE frequencies of a plot summary\n",
    "# #     params: a text called 'summary' and a set of US locations containing the cities, counties and the states\n",
    "# #     returns: all the US GPE frequencies detected in the summary\n",
    "# #     \"\"\"\n",
    "\n",
    "# def extract_GPE_and_compute_US_frequencies(summary, set_us_location):\n",
    "#     \"\"\" Function that extract the GPE entities of a plot summary\n",
    "#     params: a text called 'summary' and a set of US locations containing the cities, counties and the states\n",
    "#     returns: all the GPE entities and all the US GPE frequencies detected in the summary\n",
    "#     \"\"\"\n",
    "#     doc = nlp(summary)\n",
    "#     GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "#     GPE_frequencies_all = Counter(GPE_entities)\n",
    "#     GPE_US_frequencies = Counter(entity for entity in GPE_entities if entity in set_us_location)\n",
    "#     Percentage_US_culture = sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values()) if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "#     return GPE_entities, GPE_US_frequencies, Percentage_US_culture\n",
    "\n",
    "# # Unpacking the results of the function\n",
    "# df_plots_us_partially_movies_GPE['GPE_entities'], df_plots_us_partially_movies_GPE['GPE_US_frequencies'], df_plots_us_partially_movies_GPE['Percentage_american_culture'] = df_plots_us_partially_movies_GPE['Summary'].apply(extract_GPE_and_compute_US_frequencies)\n",
    "\n",
    "\n",
    "# # df_plots_us_partially_movies_GPE['GPE_frequencies'] = df_plots_us_partially_movies_GPE['GPE_entities'].apply(\n",
    "# #     lambda entities: Counter(entities) # use of library Counzter for efficient processing\n",
    "# # ) # ENLEVER POUR LA SUITE\n",
    "\n",
    "\n",
    "\n",
    "# # Filter GPE entities to retain only those matching US locations\n",
    "# # df_plots_us_partially_movies_GPE['GPE_US_frequencies'] = df_plots_us_partially_movies_GPE['GPE_entities'].apply(\n",
    "# #     lambda entities: Counter(entity for entity in entities if entity in us_locations_set)\n",
    "# # )\n",
    "\n",
    "# # \"Percentage\" of american culture\n",
    "# # df_plots_us_partially_movies_GPE['Percentage_american_culture'] = df_plots_us_partially_movies_GPE.apply(\n",
    "# #     lambda row: sum(row['GPE_US_frequencies'].values()) / sum(row['GPE_frequencies'].values())\n",
    "# #     if sum(row['GPE_frequencies'].values()) > 0 else 0,\n",
    "# #     axis=1\n",
    "# # )\n",
    "\n",
    "# df_plots_us_partially_movies_GPE.head()\n",
    "\n",
    "\n",
    "# mean_amercian_culture_us_partially_movies_GPE = df_plots_us_partially_movies_GPE['Percentage_american_culture'].mean()\n",
    "\n",
    "# print(f\"The mean percenatge of amercian culture of partially US-produced movies based on location only is {mean_amercian_culture_us_partially_movies_GPE*100:.2f} %.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD US lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of US lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# import wikipedia # to retrieve wikipedia page text content\n",
    "# import spacy # to implement NLP on the wikipedia page text\n",
    "\n",
    "# # Initialize the Spacy analyzer in English since all the wikipedia pages are analysed in English\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# # Function to process a Wikipedia page\n",
    "# def process_page(page_name):\n",
    "#     page_content = wikipedia.page(page_name, auto_suggest=False).content.replace('==', '').replace('\\n', '')\n",
    "#     doc = nlp(page_content) # tokenizing each page\n",
    "#     return [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha] # removing stopwords and non alphabetic characters (with .is_alpha) and lemmatize the text to discard close form of the same word\n",
    "\n",
    "# # Process each page separately for verification and clarity purposes\n",
    "# us_words = process_page('United States') # https://en.wikipedia.org/wiki/United_States \n",
    "# #fr_words = process_page('France') # https://en.wikipedia.org/wiki/France \n",
    "# uk_words = process_page('United Kingdom') # https://en.wikipedia.org/wiki/United_Kingdom \n",
    "# de_words = process_page('Germany') # https://en.wikipedia.org/wiki/Germany \n",
    "# #it_words = process_page('Italy') # https://en.wikipedia.org/wiki/Italy \n",
    "# #jp_words = process_page('Japan') # https://en.wikipedia.org/wiki/Japan \n",
    "# #ch_words = process_page('Switzerland') # https://en.wikipedia.org/wiki/Switzerland \n",
    "# ir_words = process_page('Ireland') # https://en.wikipedia.org/wiki/Ireland --> since Ireland has a strong impact on amercican culture\n",
    "# cn_words = process_page('Canada') # https://en.wikipedia.org/wiki/Canada \n",
    "# nz_words = process_page('New Zealand') # https://en.wikipedia.org/wiki/New_Zealand\n",
    "\n",
    "# # Combine words from FR, UK, DE and keep only the unique ones for faster processing\n",
    "# other_words = set( uk_words + de_words + ir_words + cn_words + nz_words)\n",
    "\n",
    "# # Extract unique US words using the set() function\n",
    "# unique_us_words = set(us_words) - other_words\n",
    "\n",
    "# print(f\"The list of unique US words is: \\n {unique_us_words}\")\n",
    "# print(f\"Unique US words: {len(unique_us_words)}\")\n",
    "# # https://www.britannica.com/place/United-States\n",
    "# # https://en.wikipedia.org/wiki/Culture_of_the_United_States \n",
    "# # intersection plutot que soustraction\n",
    "\n",
    "\n",
    "# list_straightforward_American_words = ['hollywood', 'cowboy', 'thanksgiving', 'donut', 'broadway', 'sheriff', 'mcdonald', 'doughnut', 'hamburger', 'pentagon', 'halloween', 'usa', 'U.S.']\n",
    "\n",
    "# words_in_list = []\n",
    "# words_not_in_list = []\n",
    "\n",
    "# for amercian_word in list_straightforward_American_words:\n",
    "#     if amercian_word in unique_us_words:\n",
    "#         words_in_list.append(amercian_word)\n",
    "#     else:\n",
    "#         words_not_in_list.append(amercian_word)\n",
    "\n",
    "# print(f\"The words that are both in the US wikipedia list and in the simple straightforward american list are {words_in_list}\")\n",
    "# print(f\"The words that are in the US wikipedia list but NOT in the simple straightforward american list are {words_not_in_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_trf\")\n",
    "\n",
    "# Define multi-word lexicon\n",
    "multi_word_lexicon = {\"muscle car\", \"slam dunk\"}\n",
    "\n",
    "# Create PhraseMatcher and add patterns\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = [nlp.make_doc(term) for term in multi_word_lexicon]\n",
    "matcher.add(\"US_TERMS\", patterns)\n",
    "\n",
    "# Example sentence\n",
    "text = \"I went to the slam dunk with my muscle car.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "# Find matches\n",
    "matches = matcher(doc)\n",
    "matched_terms = [doc[start:end].text.lower() for match_id, start, end in matches]\n",
    "print(matched_terms)  # Output: ['slam dunk', 'muscle car']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Test\n",
    "nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "text = \"I went with my cool friends to a Thanksgiving party after electoral college :) !! We then headed back home by taking Route 66. Indeed next day we will have electoral day in the Silicon valley.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "summary_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha]\n",
    "print(summary_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# Define the geocoding function\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    try:\n",
    "        locate = geolocator.geocode(location)\n",
    "        if locate and 'United States' in str(locate.address):\n",
    "            return location, True\n",
    "        else:\n",
    "            return location, False\n",
    "    except Exception as e:\n",
    "        # Handle any exceptions (e.g., network issues)\n",
    "        print(f\"Error geocoding {location}: {e}\")\n",
    "        return location, False\n",
    "\n",
    "# List of locations to check\n",
    "locations = [\n",
    "    \"wall street\", \"grand canyon\", \"times square\", \"alcatraz\", \"las vegas strip\", \n",
    "    \"mississippi river\", , \"brooklyn bridge\", \"mount rushmore\", \n",
    "    \"mount vernon\", \"rockefeller center\"\n",
    "]\n",
    "\n",
    "# Check each location\n",
    "results = [geocode_location(location) for location in locations]\n",
    "\n",
    "# Print results\n",
    "for location, is_us in results:\n",
    "    print(f\"{location}: {'Detected in the US' if is_us else 'Not detected in the US'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_US_lexicon_frequencies_batch(summaries, us_lexicon):\n",
    "#     \"\"\"\n",
    "#     Function that implement batch processing of plot summaries and extract GPE entities and frequencies. If available on the machine running the notebook, CUDA is enabled for faster processing.\n",
    "#     params: texts called 'summaries' and a set of US locations containing the cities, counties and the states\n",
    "#     returns: all the token entities and all the US token frequencies detected in the summary as a list\n",
    "#     \"\"\"\n",
    "#     results = []\n",
    "#     for doc in nlp.pipe(summaries, batch_size=500):\n",
    "#         summary_tokens = [token.text.lemma_.lower() for token in doc if not token.is_stop and token.is_alpha] # lemmatize, lower all letters, remove stopwords and numbers\n",
    "#         Tokens_frequencies_all = Counter(summary_tokens)\n",
    "#         Tokens_US_frequencies = Counter(token for token in summary_tokens if token in us_lexicon)\n",
    "#         Percentage_US_culture_lexicon = (\n",
    "#             sum(Tokens_US_frequencies.values()) / sum(Tokens_frequencies_all.values())\n",
    "#             if sum(Tokens_frequencies_all.values()) > 0 else 0\n",
    "#         )\n",
    "#         results.append((Tokens_frequencies_all, Tokens_US_frequencies, Percentage_US_culture_lexicon))\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test US lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Filter DataFrame for selected movie titles\n",
    "selected_titles_2 = [\"The Hunger Games\", \"Dark Water\", \"Meet John Doe\",\"Exodus\", \"Saboteur\", \"College road Trip\", \"City Hall\"] # \n",
    "filtered_df_2 = plot_summaries_us_movies[plot_summaries_us_movies['title'].isin(selected_titles_2)].copy()\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "extracted_results = compute_US_lexicon_frequencies_batch(filtered_df_2['Summary'].tolist(), unique_us_words_set)\n",
    "filtered_df_2['Total_tokens'] = [res[0] for res in extracted_results]\n",
    "filtered_df_2['Token_US_frequencies'] = [res[1] for res in extracted_results]\n",
    "filtered_df_2['Percentage_american_culture_lexicon'] = [res[2] for res in extracted_results]\n",
    "\n",
    "\n",
    "\n",
    "filtered_df_2 = filtered_df_2.drop(columns=['Summary']) # keeping all the info of the original df, the new GPE columns and dropping the summaries for more clarity and later use\n",
    "\n",
    "filtered_df_2.to_csv(\"data/NLP_datasets/NLP_US_lexicon/NLP_US_lexicon_testUSOnly.csv\", sep=',', encoding='utf-8', index=False, header = True)# hard code the encoding to avoid anay problems as seen in the lecture\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n",
    "\n",
    "filtered_df_2.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP very first tries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Disable parser and NER for better efficiency\n",
    "# Indeed the nlp function from spacy implements the whole NLP piepeline so i involves a lot of diffrent step as the following ones: \"tagger\", \"parser\", \"ner\", etc.\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "def tokenize_and_count(df):\n",
    "    \"\"\"\n",
    "    Function that counts the number of tokens per plot summaries \n",
    "    Params: df (pd.DataFrame): DataFrame containing a column 'Summary'.\n",
    "    Returns: The count of tokens for the given df.\n",
    "    \"\"\"\n",
    "    if 'Summary' not in df.columns:\n",
    "        raise ValueError(\"The dataframe must have a column named 'Summary'.\")\n",
    "    \n",
    "    # nlp.pipe for better efficiency since there are at maximum about 42'300 plot summaries to be processed\n",
    "    # it allows for parrallelize computing using batches\n",
    "    tokens = [token.text for doc in nlp.pipe(df['Summary'], batch_size=1000) for token in doc] \n",
    "    token_counts = Counter(tokens)\n",
    "    return token_counts\n",
    "\n",
    "all_movies_tokens = tokenize_and_count(plot_summaries_all_movies)\n",
    "us_only_tokens = tokenize_and_count(plot_summaries_us_movies)\n",
    "partial_us_tokens = tokenize_and_count(plot_summaries_us_partially_movies)\n",
    "rest_world_tokens = tokenize_and_count(plot_summaries_RoW_movies)\n",
    "\n",
    "# Semantic analysis based on lexical categories\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "# pre-build categores\n",
    "for cat in list(lexicon.cats.keys())[:15]:\n",
    "    print(cat) # returns words linked with cat \n",
    "\n",
    "# examine representative terms for each category\n",
    "lexicon.cats[\"health\"][:15]\n",
    "\n",
    "# analyse a whole book\n",
    "empath_features = lexicon.analyze(doc.text,categories = [\"disappointment\", \"pain\", \"joy\", \"beauty\", \"affection\"]) # returns the number of each specified categories features in the book\n",
    "\n",
    "# create ustom categories\n",
    "# my list to test\n",
    "\n",
    "lexicon.create_category(\"american_culture\", [\"New York\", \"burger\", \"guns\", \"Whashington\", \"cowboys\", \"\"], model=\"nytimes\") # model = \"nytimes\" (New York Times), \"fiction\"or \"reddit\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
