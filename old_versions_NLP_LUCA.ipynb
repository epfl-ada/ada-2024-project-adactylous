{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating deep copy of the plots_summaries dataframes creatde above \n",
    "df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "# Verify if laptop running the script has a GPU and CUDA enabled\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu() # enable GPU use for Spacy, taken from: https://stackoverflow.com/questions/75355264/how-to-enable-cuda-gpu-acceleration-for-spacy-on-windows \n",
    "    nlp = spacy.load(\"en_core_web_trf\") # uses a more complex model leveraging transformers\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# \n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries, us_locations):\n",
    "    \"\"\"\n",
    "    Function that implement batch processing of plot summaries and extract GPE entities and frequencies. If available on the machine running the notebook, CUDA is enabled for faster processing.\n",
    "    params: texts called 'summaries' and a set of US locations containing the cities, counties and the states\n",
    "    returns: all the GPE entities and all the US GPE frequencies detected in the summary as a list\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for doc in nlp.pipe(summaries, batch_size=500):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(ent for ent in GPE_entities if ent in us_locations)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "df_plots_us_partially_movies_GPE[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = \\\n",
    "    pd.DataFrame(extract_GPE_and_compute_US_frequencies_batch(\n",
    "        df_plots_us_partially_movies_GPE['Summary'].tolist(), us_locations_set # converts a Pandas Series into a Python list, as nlp.pipe expects a seqeunce of string according to the documentation https://spacy.io/api/language#pipe \n",
    "    ))\n",
    "\n",
    "df_us_partially_movies_NLP_GPE = df_plots_us_partially_movies_GPE.drop(columns=['Summary']) # keeping all the info of the original df, the new GPE columns and dropping the summaries for more clarity and later use\n",
    "\n",
    "df_us_partially_movies_NLP_GPE.to_csv(\"data/us_partially_movies_NLP_GPE.csv\", sep=',', encoding='utf-8', index=False, header = True)# hard code the encoding to avoid anay problems as seen in the lecture\n",
    "\n",
    "print(\"NLP processing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_plots_us_only_movies_GPE = plot_summaries_us_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "# Verify if laptop running the script has a GPU and CUDA enabled\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu() # enable GPU use for Spacy, taken from: https://stackoverflow.com/questions/75355264/how-to-enable-cuda-gpu-acceleration-for-spacy-on-windows \n",
    "    nlp = spacy.load(\"en_core_web_trf\") # uses a more complex model leveraging transformers\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# \n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries, us_locations):\n",
    "    \"\"\"\n",
    "    Function that implement batch processing of plot summaries and extract GPE entities and frequencies. If available on the machine running the notebook, CUDA is enabled for faster processing.\n",
    "    params: texts called 'summaries' and a set of US locations containing the cities, counties and the states\n",
    "    returns: all the GPE entities and all the US GPE frequencies detected in the summary as a list\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for doc in nlp.pipe(summaries, batch_size=200):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(ent for ent in GPE_entities if ent in us_locations)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "df_plots_us_only_movies_GPE[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = \\\n",
    "    pd.DataFrame(extract_GPE_and_compute_US_frequencies_batch(\n",
    "        df_plots_us_only_movies_GPE['Summary'].tolist(), us_locations_set # converts a Pandas Series into a Python list, as nlp.pipe expects a seqeunce of string according to the documentation https://spacy.io/api/language#pipe \n",
    "    ))\n",
    "\n",
    "df_us_only_movies_NLP_GPE = df_plots_us_only_movies_GPE.drop(columns=['Summary']) # keeping all the info of the original df, the new GPE columns and dropping the summaries for more clarity and later use\n",
    "\n",
    "df_us_only_movies_NLP_GPE.to_csv(\"data/us_only_movies_NLP_GPE.csv\", sep=',', encoding='utf-8', index=False, header = True)# hard code the encoding to avoid anay problems as seen in the lecture\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu()  # Enable GPU use for SpaCy\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # Use a more complex model leveraging transformers\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Geocode a single location and check if it's in the US\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    locate = geolocator.geocode(location)\n",
    "    check = True if locate and 'United States' in str(locate) else False\n",
    "    return location, check\n",
    "\n",
    "# Geocode multiple locations in parallel\n",
    "def geocode_locations_parallel(locations):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(geocode_location, locations), total=len(locations), desc=\"Geocoding Locations\"))\n",
    "    return results\n",
    "\n",
    "# Extract GPE entities and compute US frequencies with geocoding\n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(summaries, batch_size=500), total=len(summaries), desc=\"Processing Summaries\"):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        geocoded_results = geocode_locations_parallel(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(location for location, is_in_us in geocoded_results if is_in_us)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Example DataFrame setup and processing\n",
    "# Assuming df_plots_us_partially_movies_GPE is already defined\n",
    "df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "df_plots_us_partially_movies_GPE[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = pd.DataFrame(\n",
    "    extract_GPE_and_compute_US_frequencies_batch(\n",
    "        df_plots_us_partially_movies_GPE['Summary'].tolist()\n",
    "    ))\n",
    "\n",
    "# Drop summaries and save results\n",
    "df_us_partially_movies_NLP_GPE = df_plots_us_partially_movies_GPE.drop(columns=['Summary'])\n",
    "df_us_partially_movies_NLP_GPE.to_csv(\"data/NLP_datasets/us_partially_movies_NLP_GPE_test_problems.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST AVEC NOMINATIM Sur les plots problÃ©matiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu()  # Enable GPU use for SpaCy\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # Use a more complex model leveraging transformers (cf. https://spacy.io/models/en/)\n",
    "    print(\"Using Spacy English transformer pipeline.\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Using Spacy English pipeline optimized for CPU.\")\n",
    "\n",
    "\n",
    "# Initialize caching dictionary\n",
    "geocode_cache = {}\n",
    "\n",
    "# Geocode a single location and check if it's in the US\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    locate = geolocator.geocode(location)\n",
    "    check = True if locate and 'United States' in str(locate) else False\n",
    "    return location, check\n",
    "\n",
    "# Cached geocoding function tio avoid looking sevarl time for the same location of the geopy API\n",
    "def geocode_location_cached(location):\n",
    "    if location not in geocode_cache:\n",
    "        for attempt in range(3):  # Retry up to 3 times\n",
    "            try:\n",
    "                time.sleep(1)  # Respect API rate limits\n",
    "                geocode_cache[location] = geocode_location(location)\n",
    "                break\n",
    "            except GeocoderRateLimited:\n",
    "                print(f\"Rate-limited for location: {location}. Retrying...\")\n",
    "                time.sleep(10)  # Wait before retrying\n",
    "    return geocode_cache[location]\n",
    "\n",
    "# Geocode multiple locations in parallel\n",
    "def geocode_locations_parallel(locations):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(geocode_location_cached, locations))\n",
    "    return results\n",
    "\n",
    "# Extract GPE entities and compute US frequencies with geocoding\n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(summaries, batch_size=500), total=len(summaries), desc=\"Processing Summaries\"):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        geocoded_results = geocode_locations_parallel(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(location for location, is_in_us in geocoded_results if is_in_us)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Example DataFrame setup and processing\n",
    "# Assuming df_plots_us_partially_movies_GPE is already defined\n",
    "# df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Filter DataFrame for selected movie titles\n",
    "selected_titles_2 = [\"The Hunger Games\", \"Dark Water\", \"Meet John Doe\",\"Exodus\"] # \n",
    "filtered_df_2 = plot_summaries_us_movies[plot_summaries_us_movies['title'].isin(selected_titles_2)].copy()\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "# filtered_df[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = pd.DataFrame(\n",
    "#     extract_GPE_and_compute_US_frequencies_batch(\n",
    "#         filtered_df['Summary'].tolist()\n",
    "#     ))\n",
    "\n",
    "extracted_results = extract_GPE_and_compute_US_frequencies_batch(filtered_df_2['Summary'].tolist())\n",
    "filtered_df_2['GPE_entities'] = [res[0] for res in extracted_results]\n",
    "filtered_df_2['GPE_US_frequencies'] = [res[1] for res in extracted_results]\n",
    "filtered_df_2['Percentage_american_culture'] = [res[2] for res in extracted_results]\n",
    "\n",
    "\n",
    "# Drop summaries and save results\n",
    "filtered_df_2 = filtered_df_2.drop(columns=['Summary'])\n",
    "# filtered_df.drop(columns=['Summary'], inplace=True)\n",
    "filtered_df_2.to_csv(\"data/NLP_datasets/us_only_movies_NLP_GPE_test_problems.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n",
    "\n",
    "filtered_df_2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. Using GPU.\")\n",
    "    spacy.require_gpu()  # Enable GPU use for SpaCy\n",
    "    nlp = spacy.load(\"en_core_web_trf\")  # Use a more complex model leveraging transformers (cf. https://spacy.io/models/en/)\n",
    "    print(\"Using Spacy English transformer pipeline.\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available. Using CPU.\")\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Using Spacy English pipeline optimized for CPU.\")\n",
    "\n",
    "\n",
    "# Initialize caching dictionary\n",
    "geocode_cache = {}\n",
    "\n",
    "# Geocode a single location and check if it's in the US\n",
    "def geocode_location(location):\n",
    "    geolocator = Nominatim(user_agent=\"location_disambiguator\", timeout=10)\n",
    "    locate = geolocator.geocode(location)\n",
    "    check = True if locate and 'United States' in str(locate) else False\n",
    "    return location, check\n",
    "\n",
    "# Cached geocoding function tio avoid looking sevarl time for the same location of the geopy API\n",
    "def geocode_location_cached(location):\n",
    "    if location not in geocode_cache:\n",
    "        geocode_cache[location] = geocode_location(location)\n",
    "    return geocode_cache[location]\n",
    "\n",
    "# Geocode multiple locations in parallel\n",
    "def geocode_locations_parallel(locations):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(geocode_location_cached, locations))\n",
    "    return results\n",
    "\n",
    "# Extract GPE entities and compute US frequencies with geocoding\n",
    "def extract_GPE_and_compute_US_frequencies_batch(summaries):\n",
    "    results = []\n",
    "    for doc in tqdm(nlp.pipe(summaries, batch_size=500), total=len(summaries), desc=\"Processing Summaries\"):\n",
    "        GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "        GPE_frequencies_all = Counter(GPE_entities)\n",
    "        geocoded_results = geocode_locations_parallel(GPE_entities)\n",
    "        GPE_US_frequencies = Counter(location for location, is_in_us in geocoded_results if is_in_us)\n",
    "        Percentage_US_culture = (\n",
    "            sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values())\n",
    "            if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "        )\n",
    "        results.append((GPE_entities, GPE_US_frequencies, Percentage_US_culture))\n",
    "    return results\n",
    "\n",
    "# Example DataFrame setup and processing\n",
    "# Assuming df_plots_us_partially_movies_GPE is already defined\n",
    "# df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy()\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Processing starting.\")\n",
    "\n",
    "\n",
    "# Filter DataFrame for selected movie titles\n",
    "selected_titles = [\"Come Back, Africa\", \"A Cry in the Dark\", \"End Game\",\"Eastern Promises\",  \"Sophie's Choice\"] # \n",
    "filtered_df = plot_summaries_us_partially_movies[plot_summaries_us_partially_movies['title'].isin(selected_titles)].copy()\n",
    "\n",
    "# Apply batch processing with GPU\n",
    "# filtered_df[['GPE_entities', 'GPE_US_frequencies', 'Percentage_american_culture']] = pd.DataFrame(\n",
    "#     extract_GPE_and_compute_US_frequencies_batch(\n",
    "#         filtered_df['Summary'].tolist()\n",
    "#     ))\n",
    "\n",
    "extracted_results = extract_GPE_and_compute_US_frequencies_batch(filtered_df['Summary'].tolist())\n",
    "filtered_df['GPE_entities'] = [res[0] for res in extracted_results]\n",
    "filtered_df['GPE_US_frequencies'] = [res[1] for res in extracted_results]\n",
    "filtered_df['Percentage_american_culture'] = [res[2] for res in extracted_results]\n",
    "\n",
    "\n",
    "# Drop summaries and save results\n",
    "filtered_df = filtered_df.drop(columns=['Summary'])\n",
    "# filtered_df.drop(columns=['Summary'], inplace=True)\n",
    "filtered_df.to_csv(\"data/NLP_datasets/us_partially_movies_NLP_GPE_test_problems.csv\", sep=',', encoding='utf-8', index=False, header=True)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time/60:.2f} minutes. NLP processing done.\")\n",
    "\n",
    "print(filtered_df.columns)  \n",
    "filtered_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test avec small spacy model 'en_core_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Creating deep copy of the plots_summaries dataframes creatde above \n",
    "# df_plots_all_movies_GPE = plot_summaries_all_movies.copy()\n",
    "# df_plots_us_movies_GPE = plot_summaries_us_movies.copy()\n",
    "# df_plots_us_partially_movies_GPE = plot_summaries_us_partially_movies.copy() # run en 5'30\"\n",
    "# df_plots_RoW_GPE = plot_summaries_RoW_movies.copy()\n",
    "\n",
    "# # since GPE is part of the NER spacy pipeline, no need to tokenize manually the text before implementing the search for GPE quantities\n",
    "# # def extract_gpe(summary):\n",
    "# #     \"\"\" Function that extract the GPE entities of a plot summary\n",
    "# #     params: a text called 'summary'\n",
    "# #     returns: all the GPE entities detected in the summary\n",
    "# #     \"\"\"\n",
    "# #     doc = nlp(summary)\n",
    "# #     return [ent.text for ent in doc.ents if ent.label_ == 'GPE'] # outputs all detected geopolitical entities, including repeated mentions of the same geographical entitities\n",
    "\n",
    "# # def compute_US_GPE_frequencies(summary, set_us_location):\n",
    "# #     \"\"\" Function that extract the US GPE frequencies of a plot summary\n",
    "# #     params: a text called 'summary' and a set of US locations containing the cities, counties and the states\n",
    "# #     returns: all the US GPE frequencies detected in the summary\n",
    "# #     \"\"\"\n",
    "\n",
    "# def extract_GPE_and_compute_US_frequencies(summary, set_us_location):\n",
    "#     \"\"\" Function that extract the GPE entities of a plot summary\n",
    "#     params: a text called 'summary' and a set of US locations containing the cities, counties and the states\n",
    "#     returns: all the GPE entities and all the US GPE frequencies detected in the summary\n",
    "#     \"\"\"\n",
    "#     doc = nlp(summary)\n",
    "#     GPE_entities = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "#     GPE_frequencies_all = Counter(GPE_entities)\n",
    "#     GPE_US_frequencies = Counter(entity for entity in GPE_entities if entity in set_us_location)\n",
    "#     Percentage_US_culture = sum(GPE_US_frequencies.values()) / sum(GPE_frequencies_all.values()) if sum(GPE_frequencies_all.values()) > 0 else 0\n",
    "#     return GPE_entities, GPE_US_frequencies, Percentage_US_culture\n",
    "\n",
    "# # Unpacking the results of the function\n",
    "# df_plots_us_partially_movies_GPE['GPE_entities'], df_plots_us_partially_movies_GPE['GPE_US_frequencies'], df_plots_us_partially_movies_GPE['Percentage_american_culture'] = df_plots_us_partially_movies_GPE['Summary'].apply(extract_GPE_and_compute_US_frequencies)\n",
    "\n",
    "\n",
    "# # df_plots_us_partially_movies_GPE['GPE_frequencies'] = df_plots_us_partially_movies_GPE['GPE_entities'].apply(\n",
    "# #     lambda entities: Counter(entities) # use of library Counzter for efficient processing\n",
    "# # ) # ENLEVER POUR LA SUITE\n",
    "\n",
    "\n",
    "\n",
    "# # Filter GPE entities to retain only those matching US locations\n",
    "# # df_plots_us_partially_movies_GPE['GPE_US_frequencies'] = df_plots_us_partially_movies_GPE['GPE_entities'].apply(\n",
    "# #     lambda entities: Counter(entity for entity in entities if entity in us_locations_set)\n",
    "# # )\n",
    "\n",
    "# # \"Percentage\" of american culture\n",
    "# # df_plots_us_partially_movies_GPE['Percentage_american_culture'] = df_plots_us_partially_movies_GPE.apply(\n",
    "# #     lambda row: sum(row['GPE_US_frequencies'].values()) / sum(row['GPE_frequencies'].values())\n",
    "# #     if sum(row['GPE_frequencies'].values()) > 0 else 0,\n",
    "# #     axis=1\n",
    "# # )\n",
    "\n",
    "# df_plots_us_partially_movies_GPE.head()\n",
    "\n",
    "\n",
    "# mean_amercian_culture_us_partially_movies_GPE = df_plots_us_partially_movies_GPE['Percentage_american_culture'].mean()\n",
    "\n",
    "# print(f\"The mean percenatge of amercian culture of partially US-produced movies based on location only is {mean_amercian_culture_us_partially_movies_GPE*100:.2f} %.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
